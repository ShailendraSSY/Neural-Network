{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Activation,MaxPooling2D,Dropout,Concatenate,Input,Add\n",
    "from keras.layers.core import Dense, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint,ReduceLROnPlateau,EarlyStopping,LearningRateScheduler,CSVLogger,LambdaCallback,TensorBoard\n",
    "from keras import regularizers\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.regularizers import l2\n",
    "import seaborn as sns\n",
    "import math\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/'\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.utils import plot_model\n",
    "from sklearn.metrics import classification_report\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10514 images belonging to 2 classes.\n",
      "Found 5000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "train_path = 'cats-and-dogs/train'\n",
    "valid_path = 'cats-and-dogs/valid'\n",
    "test_path  = 'cats-and-dogs/test'\n",
    "save_path = \".\"\n",
    "train_gen = ImageDataGenerator(\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    channel_shift_range=10.,\n",
    "    fill_mode=\"nearest\"\n",
    ")\n",
    "\n",
    "train_batches = train_gen.flow_from_directory(directory=train_path, target_size=(150,150),\n",
    "    classes=['dog', 'cat'], batch_size=16)\n",
    "valid_batches = ImageDataGenerator().flow_from_directory(directory=valid_path, target_size=(150,150), \n",
    "    classes=['dog', 'cat'], batch_size=16)\n",
    "test_batches = ImageDataGenerator().flow_from_directory(directory=test_path, target_size=(150,150),\n",
    "    classes=['dog', 'cat'], batch_size=16,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_print_callback = LambdaCallback(\n",
    "    on_epoch_begin=lambda epoch,logs: print(\"LearningRate of %e\",model.optimizer.lr))\n",
    "\n",
    "earlyStopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0,\n",
    "    verbose=0,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "\tpatience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr_loss = ReduceLROnPlateau(\n",
    "    monitor='val_loss', \n",
    "\tfactor=0.1, \n",
    "    patience=7, \n",
    "\tverbose=1, \n",
    "    min_delta=1e-4,\n",
    "\tmode='auto')\n",
    "\n",
    "mcp_save       = ModelCheckpoint(\n",
    "    'cat_dog_ckpt_densenet.h5', \n",
    "\tsave_best_only=True, \n",
    "\tmonitor='val_loss', \n",
    "\tmode='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(input_,num_kernel,growth_rate,kernel_size):\n",
    "\tbn   =  BatchNormalization()(input_)\n",
    "\tact  = Activation('relu')(bn)\n",
    "\tconv = Conv2D(num_kernel, (3,3), padding='same',\n",
    "                   kernel_initializer='glorot_uniform',\n",
    "                   bias_initializer='zeros', kernel_regularizer=l2(0.0005))(act)\n",
    "\treturn conv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create a dense block which will connect a layer with all previous layer using concatenation,instead of add as in resnet'''\n",
    "def dense_block(num_dense_blk,x,nb_channels,growth_rate,kernel_size):\n",
    "    x_list = [x]\n",
    "    for i in range(num_dense_blk):\n",
    "        cb = conv_block(x,nb_channels, growth_rate,kernel_size)\n",
    "        x_list.append(cb)\n",
    "        x = Concatenate(axis=-1)(x_list)\n",
    "    nb_channels += growth_rate\n",
    "    return x, nb_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Creates a transition layer between dense blocks as transition, which do convolution and pooling.\n",
    "    Transition block uses 1x1 conv2d for downsampling'''\n",
    "def transition_block(x, nb_channels,growth_rate):\n",
    "  \n",
    "\tx = BatchNormalization()(x)\n",
    "\tx = Activation('relu')(x)\n",
    "\tx = Conv2D(nb_channels, (1, 1), padding='same',\n",
    "                      use_bias=False, kernel_regularizer=l2(0.0005))(x)\n",
    "\tx = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\tnb_channels = nb_channels + growth_rate\n",
    "\treturn x,nb_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def output_layer(x,num_kernel):\n",
    "\tx = BatchNormalization()(x)\n",
    "\tx = Activation('relu')(x)\n",
    "\tx = Conv2D(num_kernel, (3,3), padding='same',\n",
    "               kernel_initializer='glorot_uniform',\n",
    "               bias_initializer='zeros', kernel_regularizer=l2(0.0005))(x)\n",
    "\tx = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\tx = Dropout(0.5)(x)\n",
    "\tx = Flatten()(x)\n",
    "\tx = Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "\treturn x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "growth_rate =16\n",
    "nb_channels = growth_rate * 2\n",
    "visible    = Input(shape=(150, 150, 3))\n",
    "kernel_size = [(3,3),(3,3),(3,3),(3,3)]\n",
    "num_layers_dense_blk = 3\n",
    "x = Conv2D(nb_channels, (3,3), padding='same',strides=(1,1),\n",
    "                      use_bias=False, kernel_regularizer=l2(0.0005))(visible)\n",
    "for i in range(4):\n",
    "    x, nb_channels = dense_block(num_layers_dense_blk,x,nb_channels,growth_rate,kernel_size[i])\n",
    "    x, nb_channels = transition_block(x, nb_channels,growth_rate)\n",
    "x = output_layer(x,nb_channels)\n",
    "model = Model(inputs=visible, outputs=x)\n",
    "\n",
    "model.save('densenet_experiment.h5')\n",
    "model.compile(loss='binary_crossentropy',optimizer=Adam(lr=0.0001),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "658/658 [==============================] - 575s 874ms/step - loss: 1.6647 - accuracy: 0.6018 - val_loss: 1.6482 - val_accuracy: 0.6863\n",
      "Epoch 2/200\n",
      "658/658 [==============================] - 577s 877ms/step - loss: 1.5642 - accuracy: 0.6742 - val_loss: 2.5969 - val_accuracy: 0.6903\n",
      "Epoch 3/200\n",
      "658/658 [==============================] - 578s 879ms/step - loss: 1.4948 - accuracy: 0.7084 - val_loss: 1.2764 - val_accuracy: 0.7759\n",
      "Epoch 4/200\n",
      "658/658 [==============================] - 580s 881ms/step - loss: 1.4367 - accuracy: 0.7379 - val_loss: 1.1945 - val_accuracy: 0.7726\n",
      "Epoch 5/200\n",
      "658/658 [==============================] - 577s 876ms/step - loss: 1.3890 - accuracy: 0.7582 - val_loss: 1.2007 - val_accuracy: 0.8151\n",
      "Epoch 6/200\n",
      "658/658 [==============================] - 575s 874ms/step - loss: 1.3193 - accuracy: 0.7887 - val_loss: 1.1279 - val_accuracy: 0.7399\n",
      "Epoch 7/200\n",
      "658/658 [==============================] - 578s 878ms/step - loss: 1.2665 - accuracy: 0.8060 - val_loss: 1.3403 - val_accuracy: 0.7830\n",
      "Epoch 8/200\n",
      "658/658 [==============================] - 573s 871ms/step - loss: 1.2111 - accuracy: 0.8206 - val_loss: 1.0557 - val_accuracy: 0.8043\n",
      "Epoch 9/200\n",
      "658/658 [==============================] - 579s 879ms/step - loss: 1.1468 - accuracy: 0.8401 - val_loss: 0.8825 - val_accuracy: 0.8678\n",
      "Epoch 10/200\n",
      "658/658 [==============================] - 722s 1s/step - loss: 1.0911 - accuracy: 0.8584 - val_loss: 1.9603 - val_accuracy: 0.8485\n",
      "Epoch 11/200\n",
      "658/658 [==============================] - 1359s 2s/step - loss: 1.0368 - accuracy: 0.8740 - val_loss: 1.2136 - val_accuracy: 0.8844\n",
      "Epoch 12/200\n",
      "658/658 [==============================] - 1510s 2s/step - loss: 0.9962 - accuracy: 0.8799 - val_loss: 0.9373 - val_accuracy: 0.8472\n",
      "Epoch 13/200\n",
      "658/658 [==============================] - 569s 865ms/step - loss: 0.9595 - accuracy: 0.8879 - val_loss: 1.1451 - val_accuracy: 0.9040\n",
      "Epoch 14/200\n",
      "658/658 [==============================] - 583s 886ms/step - loss: 0.9087 - accuracy: 0.9018 - val_loss: 1.0057 - val_accuracy: 0.8710\n",
      "Epoch 15/200\n",
      "658/658 [==============================] - 583s 886ms/step - loss: 0.8800 - accuracy: 0.9047 - val_loss: 0.6855 - val_accuracy: 0.8996\n",
      "Epoch 16/200\n",
      "658/658 [==============================] - 583s 886ms/step - loss: 0.8478 - accuracy: 0.9113 - val_loss: 0.7044 - val_accuracy: 0.9022\n",
      "Epoch 17/200\n",
      "658/658 [==============================] - 582s 885ms/step - loss: 0.8059 - accuracy: 0.9171 - val_loss: 0.6284 - val_accuracy: 0.9010\n",
      "Epoch 18/200\n",
      "658/658 [==============================] - 582s 884ms/step - loss: 0.7873 - accuracy: 0.9197 - val_loss: 0.6195 - val_accuracy: 0.9105\n",
      "Epoch 19/200\n",
      "658/658 [==============================] - 582s 885ms/step - loss: 0.7619 - accuracy: 0.9207 - val_loss: 0.5723 - val_accuracy: 0.9330\n",
      "Epoch 20/200\n",
      "658/658 [==============================] - 582s 885ms/step - loss: 0.7338 - accuracy: 0.9250 - val_loss: 0.8943 - val_accuracy: 0.8950\n",
      "Epoch 21/200\n",
      "658/658 [==============================] - 583s 886ms/step - loss: 0.7035 - accuracy: 0.9328 - val_loss: 0.6477 - val_accuracy: 0.8912\n",
      "Epoch 22/200\n",
      "658/658 [==============================] - 583s 887ms/step - loss: 0.6794 - accuracy: 0.9342 - val_loss: 0.7932 - val_accuracy: 0.9319\n",
      "Epoch 23/200\n",
      "658/658 [==============================] - 817s 1s/step - loss: 0.6655 - accuracy: 0.9342 - val_loss: 0.4984 - val_accuracy: 0.9339\n",
      "Epoch 24/200\n",
      "658/658 [==============================] - 583s 886ms/step - loss: 0.6410 - accuracy: 0.9377 - val_loss: 0.4986 - val_accuracy: 0.9152\n",
      "Epoch 25/200\n",
      "658/658 [==============================] - 578s 878ms/step - loss: 0.6267 - accuracy: 0.9381 - val_loss: 0.7463 - val_accuracy: 0.9183\n",
      "Epoch 26/200\n",
      "658/658 [==============================] - 558s 848ms/step - loss: 0.6077 - accuracy: 0.9426 - val_loss: 0.8162 - val_accuracy: 0.9134\n",
      "Epoch 27/200\n",
      "658/658 [==============================] - 556s 844ms/step - loss: 0.5932 - accuracy: 0.9408 - val_loss: 0.4460 - val_accuracy: 0.9228\n",
      "Epoch 28/200\n",
      "658/658 [==============================] - 553s 841ms/step - loss: 0.5730 - accuracy: 0.9449 - val_loss: 0.7594 - val_accuracy: 0.9426\n",
      "Epoch 29/200\n",
      "658/658 [==============================] - 554s 842ms/step - loss: 0.5590 - accuracy: 0.9444 - val_loss: 0.4337 - val_accuracy: 0.9281\n",
      "Epoch 30/200\n",
      "658/658 [==============================] - 556s 845ms/step - loss: 0.5464 - accuracy: 0.9455 - val_loss: 0.8900 - val_accuracy: 0.9360\n",
      "Epoch 31/200\n",
      "658/658 [==============================] - 557s 846ms/step - loss: 0.5350 - accuracy: 0.9475 - val_loss: 0.5551 - val_accuracy: 0.9477\n",
      "Epoch 32/200\n",
      "658/658 [==============================] - 555s 843ms/step - loss: 0.5192 - accuracy: 0.9504 - val_loss: 0.4882 - val_accuracy: 0.9439\n",
      "Epoch 33/200\n",
      "658/658 [==============================] - 564s 856ms/step - loss: 0.5005 - accuracy: 0.9523 - val_loss: 0.3977 - val_accuracy: 0.9408\n",
      "Epoch 34/200\n",
      "658/658 [==============================] - 933s 1s/step - loss: 0.4940 - accuracy: 0.9501 - val_loss: 0.4299 - val_accuracy: 0.9408\n",
      "Epoch 35/200\n",
      "658/658 [==============================] - 1391s 2s/step - loss: 0.4802 - accuracy: 0.9526 - val_loss: 0.3988 - val_accuracy: 0.9438\n",
      "Epoch 36/200\n",
      "658/658 [==============================] - 551s 838ms/step - loss: 0.4640 - accuracy: 0.9541 - val_loss: 0.4209 - val_accuracy: 0.9340\n",
      "Epoch 37/200\n",
      "658/658 [==============================] - 4584s 7s/step - loss: 0.4626 - accuracy: 0.9561 - val_loss: 0.5994 - val_accuracy: 0.9399\n",
      "Epoch 38/200\n",
      "658/658 [==============================] - 556s 845ms/step - loss: 0.4512 - accuracy: 0.9533 - val_loss: 0.3729 - val_accuracy: 0.9401\n",
      "Epoch 39/200\n",
      "658/658 [==============================] - 567s 862ms/step - loss: 0.4377 - accuracy: 0.9590 - val_loss: 0.4139 - val_accuracy: 0.9446\n",
      "Epoch 40/200\n",
      "658/658 [==============================] - 564s 858ms/step - loss: 0.4334 - accuracy: 0.9579 - val_loss: 0.6220 - val_accuracy: 0.9277\n",
      "Epoch 41/200\n",
      "658/658 [==============================] - 570s 866ms/step - loss: 0.4170 - accuracy: 0.9606 - val_loss: 0.6568 - val_accuracy: 0.9491\n",
      "Epoch 42/200\n",
      "658/658 [==============================] - 569s 865ms/step - loss: 0.4202 - accuracy: 0.9575 - val_loss: 0.3358 - val_accuracy: 0.9135\n",
      "Epoch 43/200\n",
      "658/658 [==============================] - 561s 853ms/step - loss: 0.4022 - accuracy: 0.9615 - val_loss: 0.7775 - val_accuracy: 0.9462\n",
      "Epoch 44/200\n",
      "658/658 [==============================] - 547s 831ms/step - loss: 0.3989 - accuracy: 0.9604 - val_loss: 0.4646 - val_accuracy: 0.9610\n",
      "Epoch 45/200\n",
      "658/658 [==============================] - 546s 830ms/step - loss: 0.3944 - accuracy: 0.9630 - val_loss: 0.3669 - val_accuracy: 0.9399\n",
      "Epoch 46/200\n",
      "658/658 [==============================] - 547s 831ms/step - loss: 0.3911 - accuracy: 0.9611 - val_loss: 0.4942 - val_accuracy: 0.9418\n",
      "Epoch 47/200\n",
      "658/658 [==============================] - 1192s 2s/step - loss: 0.3760 - accuracy: 0.9644 - val_loss: 0.3893 - val_accuracy: 0.9527\n",
      "Epoch 48/200\n",
      "658/658 [==============================] - 1115s 2s/step - loss: 0.3731 - accuracy: 0.9602 - val_loss: 0.4317 - val_accuracy: 0.9463\n",
      "Epoch 49/200\n",
      "658/658 [==============================] - 843s 1s/step - loss: 0.3649 - accuracy: 0.9662 - val_loss: 0.2734 - val_accuracy: 0.9516\n",
      "Epoch 50/200\n",
      "658/658 [==============================] - 813s 1s/step - loss: 0.3603 - accuracy: 0.9638 - val_loss: 1.6070 - val_accuracy: 0.9497\n",
      "Epoch 51/200\n",
      "658/658 [==============================] - 1223s 2s/step - loss: 0.3608 - accuracy: 0.9627 - val_loss: 0.2904 - val_accuracy: 0.9554\n",
      "Epoch 52/200\n",
      "658/658 [==============================] - 554s 842ms/step - loss: 0.3557 - accuracy: 0.9622 - val_loss: 0.5560 - val_accuracy: 0.9509\n",
      "Epoch 53/200\n",
      "658/658 [==============================] - 562s 855ms/step - loss: 0.3441 - accuracy: 0.9656 - val_loss: 0.6407 - val_accuracy: 0.9448\n",
      "Epoch 54/200\n",
      "658/658 [==============================] - 568s 863ms/step - loss: 0.3409 - accuracy: 0.9648 - val_loss: 0.8068 - val_accuracy: 0.9425\n",
      "Epoch 55/200\n",
      "658/658 [==============================] - 565s 859ms/step - loss: 0.3363 - accuracy: 0.9645 - val_loss: 1.2674 - val_accuracy: 0.9339\n",
      "Epoch 56/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "658/658 [==============================] - 565s 859ms/step - loss: 0.3342 - accuracy: 0.9655 - val_loss: 0.9143 - val_accuracy: 0.9446\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "Epoch 57/200\n",
      "658/658 [==============================] - 566s 861ms/step - loss: 0.3085 - accuracy: 0.9752 - val_loss: 0.2462 - val_accuracy: 0.9590\n",
      "Epoch 58/200\n",
      "658/658 [==============================] - 568s 863ms/step - loss: 0.2940 - accuracy: 0.9809 - val_loss: 0.2442 - val_accuracy: 0.9605\n",
      "Epoch 59/200\n",
      "658/658 [==============================] - 567s 862ms/step - loss: 0.2911 - accuracy: 0.9816 - val_loss: 0.2608 - val_accuracy: 0.9603\n",
      "Epoch 60/200\n",
      "658/658 [==============================] - 566s 860ms/step - loss: 0.2838 - accuracy: 0.9839 - val_loss: 0.2483 - val_accuracy: 0.9638\n",
      "Epoch 61/200\n",
      "658/658 [==============================] - 565s 858ms/step - loss: 0.2780 - accuracy: 0.9853 - val_loss: 0.2414 - val_accuracy: 0.9655\n",
      "Epoch 62/200\n",
      "658/658 [==============================] - 564s 857ms/step - loss: 0.2775 - accuracy: 0.9868 - val_loss: 0.2404 - val_accuracy: 0.9619\n",
      "Epoch 63/200\n",
      "658/658 [==============================] - 564s 858ms/step - loss: 0.2758 - accuracy: 0.9861 - val_loss: 0.2386 - val_accuracy: 0.9660\n",
      "Epoch 64/200\n",
      "658/658 [==============================] - 564s 856ms/step - loss: 0.2732 - accuracy: 0.9864 - val_loss: 0.3853 - val_accuracy: 0.9597\n",
      "Epoch 65/200\n",
      "658/658 [==============================] - 562s 855ms/step - loss: 0.2742 - accuracy: 0.9852 - val_loss: 0.5518 - val_accuracy: 0.9665\n",
      "Epoch 66/200\n",
      "658/658 [==============================] - 565s 859ms/step - loss: 0.2686 - accuracy: 0.9883 - val_loss: 0.2375 - val_accuracy: 0.9632\n",
      "Epoch 67/200\n",
      "658/658 [==============================] - 564s 856ms/step - loss: 0.2686 - accuracy: 0.9882 - val_loss: 0.5800 - val_accuracy: 0.9691\n",
      "Epoch 68/200\n",
      "658/658 [==============================] - 562s 854ms/step - loss: 0.2649 - accuracy: 0.9892 - val_loss: 0.2464 - val_accuracy: 0.9675\n",
      "Epoch 69/200\n",
      "658/658 [==============================] - 562s 854ms/step - loss: 0.2653 - accuracy: 0.9872 - val_loss: 1.0834 - val_accuracy: 0.9635\n",
      "Epoch 70/200\n",
      "658/658 [==============================] - 561s 853ms/step - loss: 0.2679 - accuracy: 0.9877 - val_loss: 0.2369 - val_accuracy: 0.9628\n",
      "Epoch 71/200\n",
      "658/658 [==============================] - 561s 853ms/step - loss: 0.2621 - accuracy: 0.9883 - val_loss: 0.2943 - val_accuracy: 0.9650\n",
      "Epoch 72/200\n",
      "658/658 [==============================] - 561s 853ms/step - loss: 0.2582 - accuracy: 0.9903 - val_loss: 0.2339 - val_accuracy: 0.9663\n",
      "Epoch 73/200\n",
      "658/658 [==============================] - 561s 852ms/step - loss: 0.2562 - accuracy: 0.9904 - val_loss: 0.2297 - val_accuracy: 0.9625\n",
      "Epoch 74/200\n",
      "658/658 [==============================] - 561s 852ms/step - loss: 0.2585 - accuracy: 0.9891 - val_loss: 0.2419 - val_accuracy: 0.9671\n",
      "Epoch 75/200\n",
      "658/658 [==============================] - 562s 854ms/step - loss: 0.2556 - accuracy: 0.9898 - val_loss: 0.3196 - val_accuracy: 0.9651\n",
      "Epoch 76/200\n",
      "658/658 [==============================] - 562s 854ms/step - loss: 0.2555 - accuracy: 0.9892 - val_loss: 0.2420 - val_accuracy: 0.9652\n",
      "Epoch 77/200\n",
      "658/658 [==============================] - 561s 852ms/step - loss: 0.2553 - accuracy: 0.9894 - val_loss: 0.2267 - val_accuracy: 0.9655\n",
      "Epoch 78/200\n",
      "658/658 [==============================] - 561s 853ms/step - loss: 0.2519 - accuracy: 0.9909 - val_loss: 0.2479 - val_accuracy: 0.9644\n",
      "Epoch 79/200\n",
      "658/658 [==============================] - 562s 853ms/step - loss: 0.2528 - accuracy: 0.9909 - val_loss: 0.5143 - val_accuracy: 0.9622\n",
      "Epoch 80/200\n",
      "658/658 [==============================] - 562s 853ms/step - loss: 0.2495 - accuracy: 0.9910 - val_loss: 0.2245 - val_accuracy: 0.9659\n",
      "Epoch 81/200\n",
      "658/658 [==============================] - 562s 854ms/step - loss: 0.2524 - accuracy: 0.9897 - val_loss: 0.2336 - val_accuracy: 0.9659\n",
      "Epoch 82/200\n",
      "658/658 [==============================] - 562s 854ms/step - loss: 0.2476 - accuracy: 0.9916 - val_loss: 0.2321 - val_accuracy: 0.9666\n",
      "Epoch 83/200\n",
      "658/658 [==============================] - 561s 853ms/step - loss: 0.2448 - accuracy: 0.9925 - val_loss: 0.3420 - val_accuracy: 0.9675\n",
      "Epoch 84/200\n",
      "658/658 [==============================] - 562s 854ms/step - loss: 0.2474 - accuracy: 0.9914 - val_loss: 0.2547 - val_accuracy: 0.9649\n",
      "Epoch 85/200\n",
      "658/658 [==============================] - 561s 853ms/step - loss: 0.2447 - accuracy: 0.9911 - val_loss: 0.2204 - val_accuracy: 0.9647\n",
      "Epoch 86/200\n",
      "658/658 [==============================] - 561s 852ms/step - loss: 0.2431 - accuracy: 0.9922 - val_loss: 0.2418 - val_accuracy: 0.9625\n",
      "Epoch 87/200\n",
      "658/658 [==============================] - 561s 853ms/step - loss: 0.2402 - accuracy: 0.9936 - val_loss: 0.3356 - val_accuracy: 0.9679\n",
      "Epoch 88/200\n",
      "658/658 [==============================] - 562s 854ms/step - loss: 0.2405 - accuracy: 0.9926 - val_loss: 0.2219 - val_accuracy: 0.9683\n",
      "Epoch 89/200\n",
      "658/658 [==============================] - 561s 853ms/step - loss: 0.2417 - accuracy: 0.9906 - val_loss: 0.2241 - val_accuracy: 0.9630\n",
      "Epoch 90/200\n",
      "658/658 [==============================] - 562s 854ms/step - loss: 0.2415 - accuracy: 0.9911 - val_loss: 0.4028 - val_accuracy: 0.9673\n",
      "Epoch 91/200\n",
      "658/658 [==============================] - 563s 855ms/step - loss: 0.2365 - accuracy: 0.9932 - val_loss: 0.3324 - val_accuracy: 0.9652\n",
      "Epoch 92/200\n",
      "658/658 [==============================] - 565s 859ms/step - loss: 0.2363 - accuracy: 0.9932 - val_loss: 0.2935 - val_accuracy: 0.9652\n",
      "\n",
      "Epoch 00092: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "Epoch 93/200\n",
      "658/658 [==============================] - 567s 862ms/step - loss: 0.2340 - accuracy: 0.9937 - val_loss: 0.2281 - val_accuracy: 0.9660\n",
      "Epoch 94/200\n",
      "658/658 [==============================] - 566s 860ms/step - loss: 0.2358 - accuracy: 0.9924 - val_loss: 0.7287 - val_accuracy: 0.9663\n",
      "Epoch 95/200\n",
      "658/658 [==============================] - 563s 856ms/step - loss: 0.2351 - accuracy: 0.9934 - val_loss: 0.3649 - val_accuracy: 0.9657\n",
      "Epoch 96/200\n",
      "658/658 [==============================] - 563s 855ms/step - loss: 0.2361 - accuracy: 0.9916 - val_loss: 0.2150 - val_accuracy: 0.9663\n",
      "Epoch 97/200\n",
      "658/658 [==============================] - 564s 857ms/step - loss: 0.2315 - accuracy: 0.9945 - val_loss: 0.2149 - val_accuracy: 0.9669\n",
      "Epoch 98/200\n",
      "658/658 [==============================] - 564s 856ms/step - loss: 0.2310 - accuracy: 0.9939 - val_loss: 0.6509 - val_accuracy: 0.9665\n",
      "Epoch 99/200\n",
      "658/658 [==============================] - 568s 863ms/step - loss: 0.2320 - accuracy: 0.9942 - val_loss: 0.2283 - val_accuracy: 0.9660\n",
      "Epoch 100/200\n",
      "658/658 [==============================] - 571s 868ms/step - loss: 0.2340 - accuracy: 0.9932 - val_loss: 0.2147 - val_accuracy: 0.9662\n",
      "Epoch 101/200\n",
      "658/658 [==============================] - 568s 864ms/step - loss: 0.2309 - accuracy: 0.9944 - val_loss: 0.4442 - val_accuracy: 0.9668\n",
      "Epoch 102/200\n",
      "658/658 [==============================] - 564s 857ms/step - loss: 0.2338 - accuracy: 0.9939 - val_loss: 0.2286 - val_accuracy: 0.9655\n",
      "Epoch 103/200\n",
      "658/658 [==============================] - 565s 858ms/step - loss: 0.2317 - accuracy: 0.9941 - val_loss: 0.2209 - val_accuracy: 0.9670\n",
      "Epoch 104/200\n",
      "658/658 [==============================] - 564s 857ms/step - loss: 0.2311 - accuracy: 0.9942 - val_loss: 0.2252 - val_accuracy: 0.9659\n",
      "Epoch 105/200\n",
      "658/658 [==============================] - 564s 857ms/step - loss: 0.2310 - accuracy: 0.9944 - val_loss: 0.3791 - val_accuracy: 0.9662\n",
      "Epoch 106/200\n",
      "658/658 [==============================] - 564s 857ms/step - loss: 0.2299 - accuracy: 0.9940 - val_loss: 0.2616 - val_accuracy: 0.9660\n",
      "Epoch 107/200\n",
      "658/658 [==============================] - 563s 856ms/step - loss: 0.2323 - accuracy: 0.9939 - val_loss: 0.2141 - val_accuracy: 0.9659\n",
      "Epoch 108/200\n",
      "658/658 [==============================] - 563s 856ms/step - loss: 0.2298 - accuracy: 0.9943 - val_loss: 1.3481 - val_accuracy: 0.9661\n",
      "Epoch 109/200\n",
      "658/658 [==============================] - 565s 858ms/step - loss: 0.2303 - accuracy: 0.9940 - val_loss: 0.2277 - val_accuracy: 0.9659\n",
      "Epoch 110/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "658/658 [==============================] - 568s 863ms/step - loss: 0.2327 - accuracy: 0.9934 - val_loss: 1.5438 - val_accuracy: 0.9660\n",
      "Epoch 111/200\n",
      "658/658 [==============================] - 567s 861ms/step - loss: 0.2305 - accuracy: 0.9937 - val_loss: 0.2138 - val_accuracy: 0.9671\n",
      "Epoch 112/200\n",
      "658/658 [==============================] - 566s 860ms/step - loss: 0.2321 - accuracy: 0.9928 - val_loss: 0.2164 - val_accuracy: 0.9652\n",
      "Epoch 113/200\n",
      "658/658 [==============================] - 574s 872ms/step - loss: 0.2281 - accuracy: 0.9950 - val_loss: 0.2176 - val_accuracy: 0.9657\n",
      "Epoch 114/200\n",
      "658/658 [==============================] - 567s 862ms/step - loss: 0.2276 - accuracy: 0.9957 - val_loss: 0.2136 - val_accuracy: 0.9648\n",
      "Epoch 115/200\n",
      "658/658 [==============================] - 566s 860ms/step - loss: 0.2316 - accuracy: 0.9937 - val_loss: 0.2137 - val_accuracy: 0.9649\n",
      "Epoch 116/200\n",
      "658/658 [==============================] - 570s 867ms/step - loss: 0.2310 - accuracy: 0.9943 - val_loss: 0.2133 - val_accuracy: 0.9639\n",
      "Epoch 117/200\n",
      "658/658 [==============================] - 571s 867ms/step - loss: 0.2304 - accuracy: 0.9949 - val_loss: 0.2226 - val_accuracy: 0.9661\n",
      "Epoch 118/200\n",
      "658/658 [==============================] - 567s 862ms/step - loss: 0.2284 - accuracy: 0.9958 - val_loss: 0.2133 - val_accuracy: 0.9653\n",
      "Epoch 119/200\n",
      "658/658 [==============================] - 568s 863ms/step - loss: 0.2295 - accuracy: 0.9943 - val_loss: 0.2158 - val_accuracy: 0.9664\n",
      "Epoch 120/200\n",
      "658/658 [==============================] - 567s 861ms/step - loss: 0.2296 - accuracy: 0.9937 - val_loss: 0.2598 - val_accuracy: 0.9674\n",
      "Epoch 121/200\n",
      "658/658 [==============================] - 565s 858ms/step - loss: 0.2287 - accuracy: 0.9946 - val_loss: 0.4002 - val_accuracy: 0.9663\n",
      "Epoch 122/200\n",
      "658/658 [==============================] - 564s 857ms/step - loss: 0.2297 - accuracy: 0.9939 - val_loss: 0.2129 - val_accuracy: 0.9664\n",
      "Epoch 123/200\n",
      "658/658 [==============================] - 565s 859ms/step - loss: 0.2301 - accuracy: 0.9945 - val_loss: 0.2128 - val_accuracy: 0.9663\n",
      "Epoch 124/200\n",
      "658/658 [==============================] - 566s 861ms/step - loss: 0.2282 - accuracy: 0.9946 - val_loss: 0.2168 - val_accuracy: 0.9654\n",
      "Epoch 125/200\n",
      "658/658 [==============================] - 564s 858ms/step - loss: 0.2286 - accuracy: 0.9945 - val_loss: 0.2524 - val_accuracy: 0.9672\n",
      "Epoch 126/200\n",
      "658/658 [==============================] - 564s 858ms/step - loss: 0.2297 - accuracy: 0.9942 - val_loss: 0.2128 - val_accuracy: 0.9675\n",
      "Epoch 127/200\n",
      "658/658 [==============================] - 564s 857ms/step - loss: 0.2257 - accuracy: 0.9955 - val_loss: 0.8502 - val_accuracy: 0.9672\n",
      "Epoch 128/200\n",
      "658/658 [==============================] - 565s 858ms/step - loss: 0.2274 - accuracy: 0.9944 - val_loss: 0.7937 - val_accuracy: 0.9661\n",
      "Epoch 129/200\n",
      "658/658 [==============================] - 566s 860ms/step - loss: 0.2301 - accuracy: 0.9939 - val_loss: 0.5927 - val_accuracy: 0.9668\n",
      "\n",
      "Epoch 00129: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "Epoch 130/200\n",
      "658/658 [==============================] - 567s 862ms/step - loss: 0.2254 - accuracy: 0.9959 - val_loss: 0.2253 - val_accuracy: 0.9667\n",
      "Epoch 131/200\n",
      "658/658 [==============================] - 568s 864ms/step - loss: 0.2281 - accuracy: 0.9941 - val_loss: 0.2123 - val_accuracy: 0.9669\n",
      "Epoch 132/200\n",
      "658/658 [==============================] - 566s 860ms/step - loss: 0.2286 - accuracy: 0.9954 - val_loss: 0.2123 - val_accuracy: 0.9667\n",
      "Epoch 133/200\n",
      "658/658 [==============================] - 680s 1s/step - loss: 0.2270 - accuracy: 0.9955 - val_loss: 0.2289 - val_accuracy: 0.9664\n",
      "Epoch 134/200\n",
      " 84/658 [==>...........................] - ETA: 11:51 - loss: 0.2266 - accuracy: 0.9944"
     ]
    }
   ],
   "source": [
    "\n",
    "history = model.fit_generator(generator=train_batches, steps_per_epoch=len(train_batches), \n",
    "                              validation_data=valid_batches, validation_steps=len(valid_batches), \n",
    "                              epochs=200, callbacks=[reduce_lr_loss],verbose=1) \n",
    "acc = history.history['accuracy'] \n",
    "val_acc = history.history['val_accuracy'] \n",
    "loss = history.history['loss'] \n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc') \n",
    "plt.title('Training and validation accuracy') \n",
    "plt.legend() \n",
    "plt.figure() \n",
    "plt.plot(epochs, loss, 'bo', label='Training loss') \n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss') \n",
    "plt.title('Training and validation loss') \n",
    "plt.savefig('accuracy.png')\n",
    "plt.show()\n",
    "model.save('densenet_experiment.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_generator(generator=test_batches, steps=len(test_batches), verbose=0)\n",
    "pred_label = np.argmax(predictions,axis=1)\n",
    "classes = np.argmax(predictions, axis=1)\n",
    "cm = confusion_matrix(test_batches.labels,pred_label)\n",
    "f,ax = plt.subplots(figsize=(4, 4))\n",
    "sns.heatmap(cm, annot=True, linewidths=0.01,cmap=\"Greens\",linecolor=\"gray\", fmt= '.1f',ax=ax)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "plt.savefig('confusion_matrix.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import metrics\n",
    "print(classification_report(test_batches.classes, pred_label,\n",
    "\ttarget_names=test_batches.class_indices.keys()))\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "accuracy = (tn + tp)/(tn + tp + fn +fp)\n",
    "precision = precision_score(test_batches.labels, pred_label, average='binary')\n",
    "recall = recall_score(test_batches.labels, pred_label,average='binary')\n",
    "f1_score = f1_score(test_batches.labels, pred_label, average='binary')\n",
    "score = metrics.accuracy_score(test_batches.labels, pred_label)\n",
    "log_score = metrics.log_loss(pred_label, predictions)\n",
    "print(\"Precision \",precision*100)\n",
    "print(\"Recall \",recall*100)\n",
    "print(\"F1 Score \",recall*100)\n",
    "print(\"Accuracy of the model\",accuracy*100)\n",
    "print(\"Accuracy score: {}\".format(score))\n",
    "print(\"Log loss score: {}\".format(log_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "def plot_roc(pred,y):\n",
    "    fpr, tpr, _ = roc_curve(y, pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC)')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "plot_roc(pred_label,test_batches.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
